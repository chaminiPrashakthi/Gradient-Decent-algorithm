"# Gradient-Decent-algorithm" 

	Input: a starting point(current x) x 2 domf where f is a differentiable function, precision λ,
	learning rate η and let’s assume previous step size as current x initially.
	Output: some x hopefully minimizing f
	repeat
	1. previous x = current x
	2. current x current x − η ∗ rf (x)
	3. previous step size =jcurrent x − previous xj
	until previous step size > λ"# Gradient-Decent-algorithm" 
